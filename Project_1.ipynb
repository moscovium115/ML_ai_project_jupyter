{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0bad30af-9fe3-40e7-a110-05cf67720795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "np.random.seed(412)  # For reproducibility\n",
    "from scipy.sparse import csc_matrix, csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df076b-4cf9-47c9-987d-1897a92af154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44f76fff-5126-4636-8b38-8f4a47269d71",
   "metadata": {},
   "source": [
    "## Exercise 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4cbde284-fd70-451b-ba65-10f902596488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data features: \n",
      " ['android.permission.GET_ACCOUNTS'\n",
      " 'com.sonyericsson.home.permission.BROADCAST_BADGE'\n",
      " 'android.permission.READ_PROFILE' 'android.permission.MANAGE_ACCOUNTS'\n",
      " 'android.permission.WRITE_SYNC_SETTINGS'\n",
      " 'android.permission.READ_EXTERNAL_STORAGE'\n",
      " 'android.permission.RECEIVE_SMS'\n",
      " 'com.android.launcher.permission.READ_SETTINGS'\n",
      " 'android.permission.WRITE_SETTINGS'\n",
      " 'com.google.android.providers.gsf.permission.READ_GSERVICES'\n",
      " 'android.permission.DOWNLOAD_WITHOUT_NOTIFICATION'\n",
      " 'android.permission.GET_TASKS'\n",
      " 'android.permission.WRITE_EXTERNAL_STORAGE'\n",
      " 'android.permission.RECORD_AUDIO'\n",
      " 'com.huawei.android.launcher.permission.CHANGE_BADGE'\n",
      " 'com.oppo.launcher.permission.READ_SETTINGS'\n",
      " 'android.permission.CHANGE_NETWORK_STATE'\n",
      " 'com.android.launcher.permission.INSTALL_SHORTCUT'\n",
      " 'android.permission.android.permission.READ_PHONE_STATE'\n",
      " 'android.permission.CALL_PHONE' 'android.permission.WRITE_CONTACTS'\n",
      " 'android.permission.READ_PHONE_STATE'\n",
      " 'com.samsung.android.providers.context.permission.WRITE_USE_APP_FEATURE_SURVEY'\n",
      " 'android.permission.MODIFY_AUDIO_SETTINGS'\n",
      " 'android.permission.ACCESS_LOCATION_EXTRA_COMMANDS'\n",
      " 'android.permission.INTERNET'\n",
      " 'android.permission.MOUNT_UNMOUNT_FILESYSTEMS'\n",
      " 'com.majeur.launcher.permission.UPDATE_BADGE'\n",
      " 'android.permission.AUTHENTICATE_ACCOUNTS'\n",
      " 'com.htc.launcher.permission.READ_SETTINGS'\n",
      " 'android.permission.ACCESS_WIFI_STATE' 'android.permission.FLASHLIGHT'\n",
      " 'android.permission.READ_APP_BADGE' 'android.permission.USE_CREDENTIALS'\n",
      " 'android.permission.CHANGE_CONFIGURATION'\n",
      " 'android.permission.READ_SYNC_SETTINGS'\n",
      " 'android.permission.BROADCAST_STICKY'\n",
      " 'com.anddoes.launcher.permission.UPDATE_COUNT'\n",
      " 'com.android.alarm.permission.SET_ALARM'\n",
      " 'com.google.android.c2dm.permission.RECEIVE'\n",
      " 'android.permission.KILL_BACKGROUND_PROCESSES'\n",
      " 'com.sonymobile.home.permission.PROVIDER_INSERT_BADGE'\n",
      " 'com.sec.android.provider.badge.permission.READ'\n",
      " 'android.permission.WRITE_CALENDAR' 'android.permission.SEND_SMS'\n",
      " 'com.huawei.android.launcher.permission.WRITE_SETTINGS'\n",
      " 'android.permission.REQUEST_INSTALL_PACKAGES'\n",
      " 'android.permission.SET_WALLPAPER_HINTS'\n",
      " 'android.permission.SET_WALLPAPER'\n",
      " 'com.oppo.launcher.permission.WRITE_SETTINGS'\n",
      " 'android.permission.RESTART_PACKAGES'\n",
      " 'me.everything.badger.permission.BADGE_COUNT_WRITE'\n",
      " 'android.permission.ACCESS_MOCK_LOCATION'\n",
      " 'android.permission.ACCESS_COARSE_LOCATION'\n",
      " 'android.permission.READ_LOGS'\n",
      " 'com.google.android.gms.permission.ACTIVITY_RECOGNITION'\n",
      " 'com.amazon.device.messaging.permission.RECEIVE'\n",
      " 'android.permission.SYSTEM_ALERT_WINDOW'\n",
      " 'android.permission.DISABLE_KEYGUARD'\n",
      " 'android.permission.USE_FINGERPRINT'\n",
      " 'me.everything.badger.permission.BADGE_COUNT_READ'\n",
      " 'android.permission.CHANGE_WIFI_STATE' 'android.permission.READ_CONTACTS'\n",
      " 'com.android.vending.BILLING' 'android.permission.READ_CALENDAR'\n",
      " 'android.permission.RECEIVE_BOOT_COMPLETED'\n",
      " 'android.permission.WAKE_LOCK' 'android.permission.ACCESS_FINE_LOCATION'\n",
      " 'android.permission.BLUETOOTH' 'android.permission.CAMERA'\n",
      " 'com.android.vending.CHECK_LICENSE'\n",
      " 'android.permission.FOREGROUND_SERVICE'\n",
      " 'android.permission.BLUETOOTH_ADMIN' 'android.permission.VIBRATE'\n",
      " 'android.permission.NFC' 'android.permission.RECEIVE_USER_PRESENT'\n",
      " 'android.permission.CLEAR_APP_CACHE'\n",
      " 'com.android.launcher.permission.UNINSTALL_SHORTCUT'\n",
      " 'com.sec.android.iap.permission.BILLING'\n",
      " 'com.htc.launcher.permission.UPDATE_SHORTCUT'\n",
      " 'com.sec.android.provider.badge.permission.WRITE'\n",
      " 'android.permission.ACCESS_NETWORK_STATE'\n",
      " 'com.google.android.finsky.permission.BIND_GET_INSTALL_REFERRER_SERVICE'\n",
      " 'com.huawei.android.launcher.permission.READ_SETTINGS'\n",
      " 'android.permission.READ_SMS' 'android.permission.PROCESS_INCOMING_CALLS'\n",
      " 'Result'] \n",
      "\n",
      "0\n",
      "The # of ones and zeros: 29332 is the same as length of y 29332\n",
      "(29332, 86)\n"
     ]
    }
   ],
   "source": [
    "#Loading data\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Loads the data matrix X and target vector y from a CSV file\"\"\"\n",
    "   \n",
    "    # load data\n",
    "    csv_data = pd.read_csv(file_path)\n",
    "    print(f\"Data features: \\n {csv_data.columns.values} \\n\")\n",
    "\n",
    "    # Separate features (X) and target (y)\n",
    "    X = csv_data.iloc[:, :-1].values\n",
    "    y = csv_data.iloc[:, -1].values\n",
    "\n",
    "    print(y[2])\n",
    "\n",
    "    num_ones_or_zeros_y=len(y[y == 0])+len(y[y==1])\n",
    "    \n",
    "    print(f\"The # of ones and zeros: {num_ones_or_zeros_y} is the same as length of y {y.shape[0]}\")\n",
    "\n",
    "\n",
    "    # Convert y to Â±1 (y is in {0, 1})\n",
    "    y = np.where(y == 0, -1,1)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "X, y = load_data('data.csv')\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffeea053-404f-42a3-a185-e23df732d069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 29332 samples and 86 features.\n",
      "Number of malicious data points: 14700\n",
      "Number of non-malicious data points: 14632\n",
      "89.01% of X's entries are 0\n"
     ]
    }
   ],
   "source": [
    "# Display data details\n",
    "print(f\"Loaded dataset with {X.shape[0]} samples and {X.shape[1]} features.\")\n",
    "print(f\"Number of malicious data points: {np.sum(y == 1)}\")\n",
    "print(f\"Number of non-malicious data points: {np.sum(y == -1)}\")\n",
    "\n",
    "sparsity = len(X[X == 0]) / X.size * 100\n",
    "print(f\"{sparsity:.2f}% of X's entries are 0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98b394-7050-4723-9f17-56ec4061ee36",
   "metadata": {},
   "source": [
    "We dont need one hot encoding since all the data is binary. the data features and many others, are binary features. Each permission is either granted (1) or not granted (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96a474-8938-4d6a-95d6-a7e5e27d2389",
   "metadata": {},
   "source": [
    "## Exercise 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b920bf91-5047-4ade-ad82-81f6383c5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create test and training sets\n",
    "\n",
    "\n",
    "\n",
    "def split_data(X, y, r=0.5):\n",
    "    \"\"\"\n",
    "    Splits the data into training and test sets.\n",
    "    X should be in CSC (Compressed Sparse Column) format.\n",
    "    y will be returned as a dense vector.\n",
    "    r: Test size ratio (0 < r < 1).\n",
    "    \"\"\"\n",
    "    # Convert X to sparse CSC matrix (if not already sparse)\n",
    "    if not isinstance(X, csc_matrix):\n",
    "        X_sparse = csc_matrix(X)\n",
    "    else:\n",
    "        X_sparse = X\n",
    "    \n",
    "    # y is now expected to be a dense array, so no need to convert it to sparse\n",
    "    y_dense = np.array(y)\n",
    "\n",
    "    # Shuffle and split data\n",
    "    indices = np.random.permutation(X_sparse.shape[0])\n",
    "    split_index = int(X_sparse.shape[0] * (1 - r))\n",
    "\n",
    "    # Splitting X\n",
    "    X_train = X_sparse[indices[:split_index], :]\n",
    "    X_test = X_sparse[indices[split_index:], :]\n",
    "    \n",
    "    # Splitting y (kept as dense)\n",
    "    y_train = y_dense[indices[:split_index]]\n",
    "    y_test = y_dense[indices[split_index:]]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Split the dataset (50/50 split)\n",
    "X_train, X_test, y_train, y_test = split_data(X, y, r=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78627de-0449-42d0-b668-f0a72768276e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0c53543-b260-442d-bb67-27be3eb43788",
   "metadata": {},
   "source": [
    "## Exercise 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1a599bf-21de-400b-ab9c-781f9fbd4e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random classification accuracy: 69.68%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def classify(X, y, w):\n",
    "    \"\"\"Returns the number of correctly classified points using the weight vector w.\"\"\"\n",
    "    # Perform matrix-vector multiplication (X.dot(w)) and predict using the sign function\n",
    "    predictions = np.sign(X.dot(w))\n",
    "    \n",
    "    # Convert sparse vector y to dense for comparison\n",
    "    if isinstance(y, np.ndarray):  # If y is already dense\n",
    "        y_dense = y\n",
    "    else:\n",
    "        y_dense = y.toarray().flatten()  # Convert sparse matrix to dense and flatten it\n",
    "    \n",
    "    # Compare predictions with true labels and count correct classifications\n",
    "    correct = np.sum(predictions == y_dense)\n",
    "    accuracy = correct / len(y_dense)\n",
    "    \n",
    "    return correct, accuracy\n",
    "\n",
    "# Example: Try random weight vector\n",
    "w_random = np.random.randn(X_train.shape[1])  # Random weight vector of appropriate size\n",
    "correct, accuracy = classify(X_test, y_test, w_random)\n",
    "print(f\"Random classification accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044da61f-5d5a-4255-81e6-98d8107dbeb7",
   "metadata": {},
   "source": [
    "We can verify that the output makes sense for random weight vectors by calculating the mean over N trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7b1ac5c-131d-4461-87ee-be9bd036918f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average accuracy is: 0.5088715396154371\n"
     ]
    }
   ],
   "source": [
    "avg_arr=[]\n",
    "for i in range(0,100):\n",
    "    w_random = np.random.randn(X_train.shape[1])  # Random weight vector of appropriate size\n",
    "    correct,accuracy=classify(X_test,y_test,w_random)\n",
    "    avg_arr.append(accuracy)\n",
    "\n",
    "\n",
    "print(f\"average accuracy is: {np.mean(avg_arr)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c207ea-1967-4682-bc83-ef7c37d72fc1",
   "metadata": {},
   "source": [
    "As expected, we get a value around 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a109b-b3d9-4242-a509-15e3f3a9fa16",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Let \n",
    "$$\n",
    "X \\in \\mathbb{R}^{n \\times d}\n",
    "$$\n",
    "be a data matrix, \n",
    "$$\n",
    "\\underline{\\omega} \\in \\mathbb{R}^{d \\times 1}\n",
    "$$\n",
    "be the parameter vector, and \n",
    "$$\n",
    "\\underline{y} \\in \\mathbb{R}^{n \\times 1}\n",
    "$$\n",
    "be the vector of target labels.\n",
    "\n",
    "Let $\\sigma(z)$ denote the **sigmoid function**, defined as:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "We define the following cost function for logistic regression with $L_2$ regularization:\n",
    "$$\n",
    "J(\\underline{\\omega}) = \\sum_{i=1}^{n} L(y_i x_i^T \\underline{\\omega}) + \\frac{\\lambda}{2} \\| \\underline{\\omega} \\|^2\n",
    "$$\n",
    "where\n",
    "$$\n",
    "L(s) = \\log(1 + e^{-s})\n",
    "$$\n",
    "is the logistic loss function.\n",
    "\n",
    "### Gradient of the Logistic Regression Cost Function\n",
    "\n",
    "Using the chain rule, we find the expression for the gradient of $J(\\underline{\\omega})$ with respect to $\\omega_j$. The gradient of the cost function $J(\\underline{\\omega})$ with respect to $\\underline{\\omega}$ is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\underline{\\omega})}{\\partial \\omega_j} = \\lambda \\omega_j - \\left(\\sum_{i=1}^n \\frac{\\exp(-y_i x_i^T \\underline{\\omega})}{1 + \\exp(-y_i x_i^T \\underline{\\omega})} y_i x_i^T \\right) \\underline{e_j}\n",
    "$$\n",
    "\n",
    "Let $z^T$ represent the row vector in the summation above:\n",
    "$$\n",
    "z^T = \\sum_{i=1}^n \\frac{\\exp(-y_i x_i^T \\underline{\\omega})}{1 + \\exp(-y_i x_i^T \\underline{\\omega})} y_i x_i^T\n",
    "$$\n",
    "Then, the gradient can be rewritten as:\n",
    "$$\n",
    "\\nabla J(\\underline{\\omega}) = \\lambda \\underline{\\omega} - \\begin{bmatrix} z^T \\underline{e_1} \\\\ z^T \\underline{e_2} \\\\ \\vdots \\\\ z^T \\underline{e_d} \\end{bmatrix} = \\lambda \\underline{\\omega} - \\begin{bmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_d\\end{bmatrix}=\\lambda \\underline{\\omega} - \\underline{z}\n",
    "$$\n",
    "\n",
    "By transposing $z^T$ to obtain $\\underline{z}$, we get:\n",
    "$$\n",
    "\\nabla J(\\underline{\\omega}) = \\lambda \\underline{\\omega} - \\sum_{i=1}^{n} \\frac{\\exp(-y_i x_i^T \\underline{\\omega})}{1 + \\exp(-y_i x_i^T \\underline{\\omega})} y_i \\underline{x_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla J(\\underline{\\omega}) =\\lambda \\underline{\\omega} - \\sum_{i=1}^{n}  \\frac{1}{1+\\exp(y_i x_i^T \\omega) } y_i \\underline{x_i}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Since $\\frac{1}{1 + \\exp(y_i x_i^T \\underline{\\omega})} = \\sigma(-y_i x_i^T \\underline{\\omega})$, this simplifies to:\n",
    "$$\n",
    "\\nabla J(\\underline{\\omega}) = \\lambda \\underline{\\omega} - \\sum_{i=1}^{n} \\sigma(-y_i x_i^T \\underline{\\omega}) y_i \\underline{x_i}\n",
    "$$\n",
    "\n",
    "Let $u_i = \\sigma(-y_i x_i^T \\underline{\\omega}) y_i$, which essentially rescales $\\underline{y}$ by the sigmoid function. Defining a new vector $\\underline{u}$ with entries $u_i$, we can rewrite the gradient as:\n",
    "$$\n",
    "\\nabla J(\\underline{\\omega}) = \\lambda \\underline{\\omega} - \\sum_{i=1}^{n} u_i \\underline{x_i}\n",
    "$$\n",
    "\n",
    "Since $x_i^T$ represents a row of the matrix $X$, the vector $\\underline{x_i}$ is, consequently, a column of the transposed matrix $X^T$. With this observation, we can see that the sum above simplifies to a matrix-vector product: \n",
    "$$\n",
    "\\nabla J(\\underline{\\omega}) = \\lambda \\underline{\\omega} - X^T \\underline{u}\n",
    "$$\n",
    "\n",
    "where the components of $\\underline{u}$ are given by $u_i = \\sigma(-y_i x_i^T \\underline{\\omega}) y_i$. \n",
    "\n",
    "## Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b3fc0be-65cc-491c-8a66-50333fe3ecb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set classification accuracy: 94.86%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(5.2111138189042165)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "num_steps = 100\n",
    "alpha = 0.001\n",
    "reg_lambda = 10\n",
    "w = np.zeros(X.shape[1])\n",
    "for step in range(num_steps):\n",
    "    weights_vector= sigmoid(-X.dot(w) * y)\n",
    "    y_rescaled =(weights_vector)*y\n",
    "    grad=-X.T.dot(y_rescaled)\n",
    "    grad += reg_lambda * w\n",
    "    w -= alpha * grad\n",
    "\n",
    "\n",
    "\n",
    "correct, accuracy = classify(X_test, y_test, w)\n",
    "print(f\"Test set classification accuracy: {accuracy * 100:.2f}%\")\n",
    "cost = -np.sum( np.log(big_sigma) ) + 0.5*reg_lambda * w.dot(w)\n",
    "cost / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e83d841c-19a9-409c-aabd-49ce9c5b53b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set classification accuracy: 94.86%\n",
      "Cost: 5.1566\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit  # Efficient sigmoid for sparse\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Using Scipy's own Sigmoid function optimized for sparse matrices.\"\"\"\n",
    "    return expit(z)\n",
    "\n",
    "def logistic_regression(X, y, alpha=0.001, reg_lambda=10, K=100):\n",
    "    \"\"\"\n",
    "    Logistic regression using gradient descent with L2 regularization.\n",
    "    Arguments:\n",
    "    X -- sparse data matrix (CSC format)\n",
    "    y -- labels\n",
    "    alpha -- learning rate\n",
    "    reg_lambda -- regularization constant\n",
    "    K -- number of gradient descent steps\n",
    "    \n",
    "    Returns:\n",
    "    w -- weight vector\n",
    "    \"\"\"\n",
    "    # Initialize weights\n",
    "    w = np.zeros(X.shape[1])\n",
    "    \n",
    "    # Gradient descent\n",
    "    for step in range(K):\n",
    "        # Prediction and gradient calculation\n",
    "        weights_vector = sigmoid(-X.dot(w) * y)\n",
    "        y_rescaled = weights_vector * y\n",
    "        grad = -X.T.dot(y_rescaled) + reg_lambda * w\n",
    "        # Update weights\n",
    "        w -= alpha * grad\n",
    "    \n",
    "    return w\n",
    "\n",
    "\n",
    "# Assuming X_train, y_train, X_test, and y_test are already defined as sparse CSC matrices\n",
    "# Hyperparameters\n",
    "alpha = 0.001\n",
    "reg_lambda = 10\n",
    "K = 100\n",
    "\n",
    "# Train model\n",
    "w = logistic_regression(X_train, y_train, alpha, reg_lambda, K)\n",
    "\n",
    "# Evaluate on test set\n",
    "correct, accuracy = classify(X_test, y_test, w)\n",
    "print(f\"Test set classification accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Calculate cost on training set\n",
    "weights_vector = sigmoid(-X_train.dot(w) * y_train)\n",
    "cost = -np.sum(np.log(weights_vector)) + 0.5 * reg_lambda * w.dot(w)\n",
    "print(f\"Cost: {cost / len(y_train):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250bc1d-ee0b-493c-81d4-69cbebd5854a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f18691-405f-4c47-adbb-f31ed3b96a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcb3ad8c-aab6-4c51-93b4-33689124473c",
   "metadata": {},
   "source": [
    "### Exercise 6) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e2a3b-4860-4b50-8aa9-67dd4363e751",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset with fake points.\n",
    "csv_data = pd.read_csv(\"data2.csv\")\n",
    "X = csv_data.iloc[:, :-1].values\n",
    "\n",
    "# Use PCA with the number of PCA axes the same as our number of features.\n",
    "# Additionaly whiten the data, meaning dividing each column by its standard deviation.\n",
    "pca = PCA(whiten=True)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "# Affirming that each column has a standard deviation of 1.\n",
    "epsilon = 0.01\n",
    "assert np.all(np.abs(np.std(X_pca, axis=0) - 1) <= epsilon)\n",
    "\n",
    "# Calculate the norm of points in the whitened PCA basis.\n",
    "# Points with a large norm can be identified as potential outliers (refer to Lecture 8).\n",
    "norm = np.linalg.norm(X_pca, axis=1) \n",
    "\n",
    "# Assuming we know that 2000 fake datapoints were added, \n",
    "# find the norm threshold that is less than exactly 2000 datapoints\n",
    "sorted_norm = np.sort(norm)\n",
    "threshold = sorted_norm[-2000];\n",
    "outliers = norm > threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d63cb-e4b5-4091-812c-fec6f7b6eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# One assumption we made is that the PCA coordinates\n",
    "# should be normally distributed and in fact we can see this is the case.\n",
    "plt.hist(X_pca[:, 5], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Whitened PCA Axis 5 Coordinate')\n",
    "plt.xlabel('PCA Coordinate 5 Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08cdb1-97e6-4fb1-ac51-f9211d3aaf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This visualization shows the outlier points detected by thresholding the norms \n",
    "# of the whitened data, compared to the actual fake points that were added. \n",
    "# Most of the true fake points are clustered around (-1, 0) in the\n",
    "# PCA-1 vs. PCA-2 coordinate space. We can also see that most of fake points were identified.\n",
    "# Note: We found that the true outliers were the last 2000 points of X in data2.csv.\n",
    "\n",
    "ax1 = 0\n",
    "ax2 = 1\n",
    "plt.scatter(X_pca[:,ax1], X_pca[:,ax2], c=norm)\n",
    "plt.scatter(X_pca[outliers,ax1], X_pca[outliers,ax2], c=\"red\", label=\"Detected Outliers\")\n",
    "plt.scatter(X_pca[-2000:,ax1], X_pca[-2000:,ax2],10, c=\"green\", label=\"True Outliers\")\n",
    "plt.xlabel(\"PCA Axis 1\")\n",
    "plt.ylabel(\"PCA Axis 2\")\n",
    "plt.title(\"Norms of Whitened Data in PCA Basis\")\n",
    "plt.legend()\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8134877-7474-4960-9e72-96997b9f5fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake points detected vs false positive statistics.\n",
    "\n",
    "outlier_indices = np.array(np.where(outliers)).T\n",
    "number_fake_detected = np.sum(outlier_indices > X.shape[0]-2000)\n",
    "print(f\"Number of fake points detected: {number_fake_detected}\")\n",
    "print(f\"Number of real points falsely detected as fake: {2000 - number_fake_detected}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
